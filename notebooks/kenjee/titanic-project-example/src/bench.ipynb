{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Project Example Walk Through \n",
    "In this notebook, I hope to show how a data scientist would go about working through a problem. The goal is to correctly predict if someone survived the Titanic shipwreck. I thought it would be fun to see how well I could do in this competition without deep learning. \n",
    "\n",
    "*The accompanying video is located here:* https://www.youtube.com/watch?v=I3FBJdiExcg\n",
    "\n",
    "**Best results : 79.425 % accuracy (Top 12%)**\n",
    "\n",
    "## Overview \n",
    "### 1) Understand the shape of the data (Histograms, box plots, etc.)\n",
    "\n",
    "### 2) Data Cleaning \n",
    "\n",
    "### 3) Data Exploration\n",
    "\n",
    "### 4) Feature Engineering \n",
    "\n",
    "### 5) Data Preprocessing for Model\n",
    "\n",
    "### 6) Basic Model Building \n",
    "\n",
    "### 7) Model Tuning \n",
    "\n",
    "### 8) Ensemble Modle Building \n",
    "\n",
    "### 9) Results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recently did something similar with the **Housing Prices Dataset for Regression** if you want something slighty more advanced. I show how I would integrate ChatGPT with my Data Science work:\n",
    "\n",
    "- Kaggle notebook: https://www.kaggle.com/code/kenjee/housing-prices-example-with-video-walkthrough\n",
    "- YouTube Video: https://www.youtube.com/watch?v=NQQ3DRdXAXE&t=2s&ab_channel=KenJee\n",
    "\n",
    "My github repos with additional free and paid resources: \n",
    "- ML Process: https://github.com/PlayingNumbers/ML_Process_Course\n",
    "- ML ALgorithms: https://github.com/PlayingNumbers/ML_Algorithms_Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/test.csv\n",
      "../input/gender_submission.csv\n",
      "../input/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "        \n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the data. For this analysis, we will be exclusively working with the Training set. We will be validating based on data from the training set as well. For our final submissions, we will make predictions based on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'train_test'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "training['train_test'] = 1\n",
    "test['train_test'] = 0\n",
    "test['Survived'] = np.NaN\n",
    "all_data = pd.concat([training,test])\n",
    "\n",
    "all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning\n",
    "When starting any project, I like to outline the steps that I plan to take. Below is the rough outline that I created for this project using commented cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand nature of the data .info() .describe()\n",
    "# Histograms and boxplots \n",
    "# Value counts \n",
    "# Missing data \n",
    "# Correlation between the metrics \n",
    "# Explore interesting themes \n",
    "    # Wealthy survive? \n",
    "    # By location \n",
    "    # Age scatterplot with ticket price \n",
    "    # Young and wealthy Variable? \n",
    "    # Total spent? \n",
    "# Feature engineering \n",
    "# preprocess data together or use a transformer? \n",
    "    # use label for train and test   \n",
    "# Scaling?\n",
    "\n",
    "# Model Baseline \n",
    "# Model comparison with CV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Data Exploration\n",
    "### 1) For numeric data \n",
    "* Made histograms to understand distributions \n",
    "* Corrplot \n",
    "* Pivot table comparing survival rate across numeric variables \n",
    "\n",
    "\n",
    "### 2) For Categorical Data \n",
    "* Made bar charts to understand balance of classes \n",
    "* Made pivot tables to understand relationship with survival \n",
    "\n",
    "#### Want more details on EDA? Check out this notebook: https://www.kaggle.com/code/kenjee/basic-eda-example-section-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      " 12  train_test   891 non-null    int64  \n",
      "dtypes: float64(2), int64(6), object(5)\n",
      "memory usage: 90.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#quick look at our data types & null counts \n",
    "training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  train_test  \n",
       "count  891.000000  891.000000       891.0  \n",
       "mean     0.381594   32.204208         1.0  \n",
       "std      0.806057   49.693429         0.0  \n",
       "min      0.000000    0.000000         1.0  \n",
       "25%      0.000000    7.910400         1.0  \n",
       "50%      0.000000   14.454200         1.0  \n",
       "75%      0.000000   31.000000         1.0  \n",
       "max      6.000000  512.329200         1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to better understand the numeric data, we want to use the .describe() method. This gives us an understanding of the central tendencies of the data \n",
    "training.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n",
       "       'train_test'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick way to separate numeric columns\n",
    "training.describe().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at numeric and categorical values separately \n",
    "df_num = training[['Age','SibSp','Parch','Fare']]\n",
    "df_cat = training[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we should take the non-normal distributions and consider normalizing them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabin and ticket graphs are very messy. This is an area where we may want to do some feature engineering! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass      1   2    3\n",
      "Survived              \n",
      "0          80  97  372\n",
      "1         136  87  119\n",
      "\n",
      "Sex       female  male\n",
      "Survived              \n",
      "0             81   468\n",
      "1            233   109\n",
      "\n",
      "Embarked   C   Q    S\n",
      "Survived             \n",
      "0         75  47  427\n",
      "1         93  30  217\n"
     ]
    }
   ],
   "source": [
    "# Comparing survival and each of these categorical variables \n",
    "print(pd.pivot_table(training, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\n",
    "print()\n",
    "print(pd.pivot_table(training, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\n",
    "print()\n",
    "print(pd.pivot_table(training, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "### 1) Cabin - Simplify cabins (evaluated if cabin letter (cabin_adv) or the purchase of tickets across multiple cabins (cabin_multiple) impacted survival)\n",
    "\n",
    "### 2) Tickets - Do different ticket types impact survival rates?\n",
    "\n",
    "### 3) Does a person's title relate to survival rates? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cabin_multiple\n",
       "0    687\n",
       "1    180\n",
       "2     16\n",
       "3      6\n",
       "4      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat.Cabin\n",
    "training['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n",
    "# after looking at this, we may want to look at cabin by letter or by number. Let's create some categories for this \n",
    "# letters \n",
    "# multiple letters \n",
    "training['cabin_multiple'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cabin_multiple</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "cabin_multiple      0      1    2    3    4\n",
       "Survived                                   \n",
       "0               481.0   58.0  7.0  3.0  NaN\n",
       "1               206.0  122.0  9.0  3.0  2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(training, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket' ,aggfunc ='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates categories based on the cabin letter (n stands for null)\n",
    "#in this case we will treat null values like it's own category\n",
    "\n",
    "training['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cabin_adv\n",
      "n    687\n",
      "C     59\n",
      "B     47\n",
      "D     33\n",
      "E     32\n",
      "A     15\n",
      "F     13\n",
      "G      4\n",
      "T      1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cabin_adv</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>T</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "cabin_adv    A     B     C     D     E    F    G    T      n\n",
       "Survived                                                    \n",
       "0          8.0  12.0  24.0   8.0   8.0  5.0  2.0  1.0  481.0\n",
       "1          7.0  35.0  35.0  25.0  24.0  8.0  2.0  NaN  206.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comparing surivial rate by cabin\n",
    "print(training.cabin_adv.value_counts())\n",
    "pd.pivot_table(training,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understand ticket values better \n",
    "#numeric vs non numeric \n",
    "training['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "training['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numeric_ticket\n",
       "1    661\n",
       "0    230\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['numeric_ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_letters\n",
       "0            665\n",
       "pc            60\n",
       "ca            41\n",
       "a5            21\n",
       "stono2        18\n",
       "sotonoq       15\n",
       "scparis       11\n",
       "wc            10\n",
       "a4             7\n",
       "soc            6\n",
       "fcc            5\n",
       "c              5\n",
       "sopp           3\n",
       "pp             3\n",
       "wep            3\n",
       "ppp            2\n",
       "scah           2\n",
       "sotono2        2\n",
       "swpp           2\n",
       "fc             1\n",
       "scahbasle      1\n",
       "as             1\n",
       "sp             1\n",
       "sc             1\n",
       "scow           1\n",
       "fa             1\n",
       "sop            1\n",
       "sca4           1\n",
       "casoton        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets us view all rows in dataframe through scrolling. This is for convenience \n",
    "training['ticket_letters'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>numeric_ticket</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "numeric_ticket    0    1\n",
       "Survived                \n",
       "0               142  407\n",
       "1                88  254"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#difference in numeric vs non-numeric tickets in survival rate \n",
    "pd.pivot_table(training,index='Survived',columns='numeric_ticket', values = 'Ticket', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticket_letters</th>\n",
       "      <th>0</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>as</th>\n",
       "      <th>c</th>\n",
       "      <th>ca</th>\n",
       "      <th>casoton</th>\n",
       "      <th>fa</th>\n",
       "      <th>fc</th>\n",
       "      <th>fcc</th>\n",
       "      <th>...</th>\n",
       "      <th>soc</th>\n",
       "      <th>sop</th>\n",
       "      <th>sopp</th>\n",
       "      <th>sotono2</th>\n",
       "      <th>sotonoq</th>\n",
       "      <th>sp</th>\n",
       "      <th>stono2</th>\n",
       "      <th>swpp</th>\n",
       "      <th>wc</th>\n",
       "      <th>wep</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ticket_letters      0   a4    a5   as    c    ca  casoton   fa   fc  fcc  ...  \\\n",
       "Survived                                                                  ...   \n",
       "0               410.0  7.0  19.0  1.0  3.0  27.0      1.0  1.0  1.0  1.0  ...   \n",
       "1               255.0  NaN   2.0  NaN  2.0  14.0      NaN  NaN  NaN  4.0  ...   \n",
       "\n",
       "ticket_letters  soc  sop  sopp  sotono2  sotonoq   sp  stono2  swpp   wc  wep  \n",
       "Survived                                                                       \n",
       "0               5.0  1.0   3.0      2.0     13.0  1.0    10.0   NaN  9.0  2.0  \n",
       "1               1.0  NaN   NaN      NaN      2.0  NaN     8.0   2.0  1.0  1.0  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#survival rate across different tyicket types \n",
    "pd.pivot_table(training,index='Survived',columns='ticket_letters', values = 'Ticket', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering on person's title \n",
    "training.Name.head(50)\n",
    "training['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "#mr., ms., master. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name_title\n",
       "Mr              517\n",
       "Miss            182\n",
       "Mrs             125\n",
       "Master           40\n",
       "Dr                7\n",
       "Rev               6\n",
       "Mlle              2\n",
       "Major             2\n",
       "Col               2\n",
       "the Countess      1\n",
       "Capt              1\n",
       "Ms                1\n",
       "Sir               1\n",
       "Lady              1\n",
       "Mme               1\n",
       "Don               1\n",
       "Jonkheer          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['name_title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Model \n",
    "### 1) Drop null values from Embarked (only 2) \n",
    "\n",
    "### 2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with) \n",
    "Variables:  'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n",
    "\n",
    "### 3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder). \n",
    "\n",
    "### 4) Impute data with mean for fare and age (Should also experiment with median) \n",
    "\n",
    "### 5) Normalized fare using logarithm to give more semblance of a normal distribution \n",
    "\n",
    "### 6) Scaled data 0-1 with standard scaler \n",
    "\n",
    "## Want more details on Feature Engineering? Check out this notebook: https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create all categorical variables that we did above for both training and test sets \n",
    "all_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n",
    "all_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\n",
    "all_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "all_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n",
    "all_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "\n",
    "#impute nulls for continuous data \n",
    "#all_data.Age = all_data.Age.fillna(training.Age.mean())\n",
    "all_data.Age = all_data.Age.fillna(training.Age.median())\n",
    "#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\n",
    "all_data.Fare = all_data.Fare.fillna(training.Fare.median())\n",
    "\n",
    "#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \n",
    "all_data.dropna(subset=['Embarked'],inplace = True)\n",
    "\n",
    "#tried log norm of sibsp (not used)\n",
    "all_data['norm_sibsp'] = np.log(all_data.SibSp+1)\n",
    "# all_data['norm_sibsp'].hist()\n",
    "\n",
    "# log norm of fare (used)\n",
    "all_data['norm_fare'] = np.log(all_data.Fare+1)\n",
    "# all_data['norm_fare'].hist()\n",
    "\n",
    "# converted fare to category for pd.get_dummies()\n",
    "all_data.Pclass = all_data.Pclass.astype(str)\n",
    "\n",
    "#created dummy variables from categories (also can use OneHotEncoder)\n",
    "all_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n",
    "\n",
    "#Split to train test again\n",
    "X_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\n",
    "X_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n",
    "\n",
    "\n",
    "y_train = all_data[all_data.train_test==1].Survived\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damitha/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "# Scale data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "all_dummies_scaled = all_dummies.copy()\n",
    "all_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\n",
    "all_dummies_scaled\n",
    "\n",
    "X_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\n",
    "X_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n",
    "\n",
    "y_train = all_data[all_data.train_test==1].Survived\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building (Baseline Validation Performance)\n",
    "Before going further, I like to see how various different models perform with default parameters. I tried the following models using 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn't mean that it will actually do better on the eventual test set. \n",
    "\n",
    "- Naive Bayes (72.6%)\n",
    "- Logistic Regression (82.1%)\n",
    "- Decision Tree (77.6%)\n",
    "- K Nearest Neighbor (80.5%)\n",
    "- Random Forest (80.6%)\n",
    "- **Support Vector Classifier (83.2%)**\n",
    "- Xtreme Gradient Boosting (81.8%)\n",
    "- Soft Voting Classifier - All Models (82.8%)\n",
    "\n",
    "\n",
    "#### Want more details on Model Tuning? Check out this notebook: https://www.kaggle.com/code/kenjee/model-building-example-section-11\n",
    "#### Want more details on Model Evaluation? Check out this notebook: https://www.kaggle.com/code/kenjee/model-evaluation-classification-section-12\n",
    "#### Want more details on Classification Algorithms? Check out this notebook: https://www.kaggle.com/code/kenjee/exhaustive-classification-parameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuned Performance \n",
    "After getting the baselines, let's see if we can improve on the indivdual model results!I mainly used grid search to tune the models. I also used Randomized Search for the Random Forest and XG boosted model to simplify testing time. \n",
    "\n",
    "|Model|Baseline|Tuned Performance|\n",
    "|-----|--------|-----------------|\n",
    "|Naive Bayes| 72.6%| NA|\n",
    "|Logistic Regression| 82.1%| 82.6%|\n",
    "|Decision Tree| 77.6%| NA|\n",
    "|K Nearest Neighbor| 80.5%|83.0%|\n",
    "|Random Forest| 80.6%| 83.6|\n",
    "|Support Vector Classifier| 83.2%| 83.2%|\n",
    "|Xtreme Gradient Boosting| 81.8%| 85.3%|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nrf = RandomForestClassifier(random_state = 1)\\nparam_grid =  {'n_estimators': [100,500,1000], \\n                                  'bootstrap': [True,False],\\n                                  'max_depth': [3,5,10,20,50,75,100,None],\\n                                  'max_features': ['auto','sqrt'],\\n                                  'min_samples_leaf': [1,2,4,10],\\n                                  'min_samples_split': [2,5,10]}\\n                                  \\nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train_scaled,y_train)\\nclf_performance(best_clf_rf_rnd,'Random Forest')\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Because the total feature space is so large, I used a randomized search to narrow down the paramters for the model. I took the best model from this and did a more granular search \n",
    "\"\"\"\n",
    "rf = RandomForestClassifier(random_state = 1)\n",
    "param_grid =  {'n_estimators': [100,500,1000], \n",
    "                                  'bootstrap': [True,False],\n",
    "                                  'max_depth': [3,5,10,20,50,75,100,None],\n",
    "                                  'max_features': ['auto','sqrt'],\n",
    "                                  'min_samples_leaf': [1,2,4,10],\n",
    "                                  'min_samples_split': [2,5,10]}\n",
    "                                  \n",
    "clf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_rf_rnd = clf_rf_rnd.fit(X_train_scaled,y_train)\n",
    "clf_performance(best_clf_rf_rnd,'Random Forest')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xgb = XGBClassifier(random_state = 1)\\n\\nparam_grid = {\\n    'n_estimators': [20, 50, 100, 250, 500,1000],\\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\\n    'reg_alpha': [0, 0.5, 1],\\n    'reg_lambda': [1, 1.5, 2],\\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\\n    'gamma':[0,.01,.1,1,10,100],\\n    'min_child_weight':[0,.01,0.1,1,10,100],\\n    'sampling_method': ['uniform', 'gradient_based']\\n}\\n\\n#clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\\n#best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\\n#clf_performance(best_clf_xgb,'XGB')\\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scaled,y_train)\\nclf_performance(best_clf_xgb_rnd,'XGB')\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"xgb = XGBClassifier(random_state = 1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 50, 100, 250, 500,1000],\n",
    "    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n",
    "    'max_depth': [2, 5, 10, 15, 20, 25, None],\n",
    "    'reg_alpha': [0, 0.5, 1],\n",
    "    'reg_lambda': [1, 1.5, 2],\n",
    "    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n",
    "    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n",
    "    'gamma':[0,.01,.1,1,10,100],\n",
    "    'min_child_weight':[0,.01,0.1,1,10,100],\n",
    "    'sampling_method': ['uniform', 'gradient_based']\n",
    "}\n",
    "\n",
    "#clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "#best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n",
    "#clf_performance(best_clf_xgb,'XGB')\n",
    "clf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scaled,y_train)\n",
    "clf_performance(best_clf_xgb_rnd,'XGB')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Additional Ensemble Approaches \n",
    "1) Experimented with a hard voting classifier of three estimators (KNN, SVM, RF) (81.6%)\n",
    "\n",
    "2) **Experimented with a soft voting classifier of three estimators (KNN, SVM, RF) (82.3%) (Best Performance)**\n",
    "\n",
    "3) Experimented with soft voting on all estimators performing better than 80% except xgb (KNN, RF, LR, SVC) (82.9%)\n",
    "\n",
    "4) Experimented with soft voting on all estimators including XGB (KNN, SVM, RF, LR, XGB) (83.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out these additional Resources for continued learning! \n",
    "\n",
    "### Hoursing Prices Walkthrough \n",
    "- Kaggle notebook: https://www.kaggle.com/code/kenjee/housing-prices-example-with-video-walkthrough\n",
    "- YouTube Video: https://www.youtube.com/watch?v=NQQ3DRdXAXE&t=2s&ab_channel=KenJee\n",
    "\n",
    "### My Other Notebooks for Learning the ML Process\n",
    "- [**Dealing with Missing Values - Section 5.1**](https://www.kaggle.com/code/kenjee/dealing-with-missing-values-section-5-1)\n",
    "- [**Dealing with Outliers - Section 5.2**](https://www.kaggle.com/code/kenjee/dealing-with-outliers-section-5-2)\n",
    "- [**Basic EDA Example - Section 6**](https://www.kaggle.com/code/kenjee/basic-eda-example-section-6)\n",
    "- [**Categorical Feature Engineering - Section 7.1**](https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1)\n",
    "- [**Numeric Feature Engineering - Section 7.2**](https://www.kaggle.com/kenjee/numeric-feature-engineering-section-7-2)\n",
    "- [**Cross Validation Foundations - Section 8**](https://www.kaggle.com/code/kenjee/cross-validation-foundations-section-8)\n",
    "- [**Feature Selection - Section 9**](https://www.kaggle.com/code/kenjee/feature-selection-section-9)\n",
    "- [**Dealing with Imbalanced Data - Section 10**](https://www.kaggle.com/code/kenjee/dealing-with-imbalanced-data-section-10)\n",
    "- [**Model Building Example - Section 11**](https://www.kaggle.com/code/kenjee/model-building-example-section-11)\n",
    "- [**Model Evaluation (Classification) - Section 11**](https://www.kaggle.com/code/kenjee/model-evaluation-classification-section-12)\n",
    "- [**Model Evlauation (Regression) - Section 11**](https://www.kaggle.com/code/kenjee/model-evaluation-regression-12)\n",
    "\n",
    "### My Other Notebooks for Learning ML Algorithms\n",
    "- [**Exhaustive Regression with Parameter Tuning**](https://www.kaggle.com/code/kenjee/exhaustive-regression-parameter-tuning)\n",
    "- [**Exhaustive Classification with Parameter Tuning**](https://www.kaggle.com/code/kenjee/exhaustive-classification-parameter-tuning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
